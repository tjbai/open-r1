model_name_or_path: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
model_revision: main
torch_dtype: bfloat16
attn_implementation: sdpa
bf16: true
use_vllm: false
use_liger_kernel: true

to_choreo: true
choreo_k: 1

# the holy seed
seed: 42

dataset_name: open-r1/OpenR1-Math-220k
dataset_prompt_column: problem

system_prompt: "You are a helpful AI Assistant that provides well-reasoned and detailed responses.
                You first think about the reasoning process as an internal monologue and then
                provide the user with the answer. Respond in the following format:
                <think>\n...\n</think>\n
                <answer>\n...\n</answer>"

chat_template: |
  {# initialize generation-flag if absent #}
  {% if not add_generation_prompt is defined %}
    {% set add_generation_prompt = false %}
  {% endif %}

  {# namespace vars to track state #}
  {% set ns = namespace(
    is_first=false,
    is_tool=false,
    is_output_first=true,
    system_prompt=''
  ) %}

  {# extract system prompt from messages #}
  {%- for message in messages %}
    {%- if message.role == 'system' %}
      {% set ns.system_prompt = message.content %}
    {%- endif %}
  {%- endfor %}

  {{ bos_token }}{{ ns.system_prompt }}

  {# loop over all messages #}
  {%- for message in messages %}

    {# user messages #}
    {%- if message.role == 'user' %}
      {% set ns.is_tool = false %}
      <｜User｜>{{ message.content }}

    {# assistant → tool call #}
    {%- elif message.role == 'assistant' and message.content is none %}
      {% set ns.is_tool = false %}
      {%- for tool in message.tool_calls %}
        {%- if not ns.is_first %}
          <｜Assistant｜>
          <｜tool▁calls▁begin｜>
          <｜tool▁call▁begin｜>
          {{ tool.type }}<｜tool▁sep｜>{{ tool.function.name }}
          ```json
          {{ tool.function.arguments }}
          ```
          <｜tool▁call▁end｜>
          {% set ns.is_first = true %}
        {%- else %}
          \n<｜tool▁call▁begin｜>
          {{ tool.type }}<｜tool▁sep｜>{{ tool.function.name }}
          ```json
          {{ tool.function.arguments }}
          ```
          <｜tool▁call▁end｜>
          <｜tool▁calls▁end｜>
          <｜end▁of▁sentence｜>
        {%- endif %}
      {%- endfor %}

    {# assistant → normal reply #}
    {%- elif message.role == 'assistant' and message.content is not none %}
      {%- if ns.is_tool %}
        <｜tool▁outputs▁end｜>{{ message.content }}<｜end▁of▁sentence｜>
        {% set ns.is_tool = false %}
      {%- else %}
        <｜Assistant｜>{{ message.content }}<｜end▁of▁sentence｜>
      {%- endif %}

    {# tool outputs #}
    {%- elif message.role == 'tool' %}
      {% set ns.is_tool = true %}
      {%- if ns.is_output_first %}
        <｜tool▁outputs▁begin｜>
        <｜tool▁output▁begin｜>{{ message.content }}<｜tool▁output▁end｜>
        {% set ns.is_output_first = false %}
      {%- else %}
        \n<｜tool▁output▁begin｜>{{ message.content }}<｜tool▁output▁end｜>
      {%- endif %}

    {%- endif %}
  {%- endfor %}

  {# close any open tool outputs #}
  {% if ns.is_tool %}
    <｜tool▁outputs▁end｜>
  {% endif %}

  {# optionally add an empty assistant prompt to trigger generation #}
  {% if add_generation_prompt and not ns.is_tool %}
    <｜Assistant｜>
  {% endif %}

do_eval: false
per_device_train_batch_size: 8
per_device_eval_batch_size: 16
gradient_accumulation_steps: 8
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
num_train_epochs: 1
max_steps: -1
learning_rate: 1e-6
lr_scheduler:
  type: cosine_with_min_lr
  kwargs:
    min_lr_rate: 0.1
warmup_ratio: 0.1

temperature: 0.7
num_generations: 16
max_prompt_length: 512
max_completion_length: 2048

reward_funcs:
  - accuracy
  - format
  - tag_count
reward_weights:
  - 1.0
  - 1.0
  - 1.0

log_level: info
logging_strategy: "steps"
logging_steps: 10
logging_first_step: true
log_completions: true
num_completions_to_print: 5

push_to_hub: true
hub_model_id: distqwen-1.5B-sanity
hub_strategy: every_save

output_dir: data/DeepSeek-R1-Distill-Qwen-1.5B-GRPO
overwrite_output_dir: true
save_strategy: epoch
save_total_limit: 1

report_to:
  - wandb
wandb_project: rlfun
